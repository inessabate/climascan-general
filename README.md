# Climascan Data Pipeline

Este repositorio implementa un pipeline de datos diseÃ±ado para construir un modelo de predicciÃ³n de riesgo ante eventos climÃ¡ticos extremos. El proyecto sigue una estructura basada en el estÃ¡ndar [Cookiecutter Data Science](https://drivendata.github.io/cookiecutter-data-science/), adaptada a un flujo de trabajo con Delta Lake, datos meteorolÃ³gicos y datos de siniestros.

---
## ğŸ¯ Objetivo del Repositorio

Este repositorio tiene como objetivo **recoger, procesar y almacenar datos** relacionados con eventos climÃ¡ticos extremos y siniestros asociados, con el fin de construir un conjunto de datos limpio y estructurado que sirva como base para el entrenamiento de modelos de predicciÃ³n de riesgo.

El flujo general del pipeline incluye:

- ğŸ“¥ **Ingesta** de datos meteorolÃ³gicos desde APIs y datos de siniestros desde ficheros Excel.
- ğŸ§¹ **Procesamiento y transformaciÃ³n** de los datos para limpieza, enriquecimiento y creaciÃ³n de variables.
- ğŸ’¾ **Almacenamiento** en formato Delta Lake para consultas eficientes y uso posterior en modelado predictivo.
---


## ğŸ“ Estructura del Proyecto

```plaintext
climascan-data-pipeline/
â”‚
â”œâ”€â”€ README.md                  # DocumentaciÃ³n del proyecto
â”œâ”€â”€ main.py                    # Orquestador principal del pipeline
â”‚
â”œâ”€â”€ config/                    # Archivos de configuraciÃ³n global
â”‚   â”œâ”€â”€ config.yaml            # ParÃ¡metros del pipeline (rutas, fechas, etc.)
â”‚   â””â”€â”€ logging.yaml           # ConfiguraciÃ³n del sistema de logging
â”‚
â”œâ”€â”€ data/                      # Almacenamiento de datos en Delta Lake
â”‚   â”œâ”€â”€ landing/               # Datos crudos (respuestas JSON). No se incluyen datos de siniestros por razones de privacidad.
â”‚   â”‚   â””â”€â”€ aemet/             # Datos crudos de la API de AEMET (JSON). Una carpeta por aÃ±o.
â”‚   â”‚  
â”‚   â”œâ”€â”€ trusted/               # Datos limpios y validados
â”‚   â”‚   â”œâ”€â”€ claims/            # Datos de siniestros en formato tabular con data quality aplicado
â”‚   â”‚   â””â”€â”€ meteo/             # Datos meteorolÃ³gicos en formato tabular con data quality aplicado
â”‚   â”‚ 
â”‚   â””â”€â”€ aggregated/            # Datos agregados en Delta Lake, listos para data analytics
â”‚       â”œâ”€â”€ claims/            # Datos de siniestros en Delta Lake
â”‚       â””â”€â”€ meteo/             # Datos meteorolÃ³gicos en Delta Lake
â”‚
â”‚
â”œâ”€â”€ notebooks/                 # ExploraciÃ³n y anÃ¡lisis 
â”‚   â”œâ”€â”€ 01_data_analytics/     # AnÃ¡lisis, consultas y features
â”‚   â””â”€â”€ 02_visualization/      # Prototipos de visualizaciÃ³n y dashboards
â”‚
â”œâ”€â”€ requirements.txt           # Dependencias del entorno Python
â”‚
â”œâ”€â”€ src/                       # CÃ³digo fuente del proyecto
â”‚   â”œâ”€â”€ data_management/       # MÃ³dulo 1ï¸âƒ£: IngestiÃ³n y procesamiento de datos
â”‚   â”‚   â”œâ”€â”€ ingestion/         # Llamadas a APIs y carga de datos
â”‚   â”‚   â”œâ”€â”€ processing/        # Limpieza y ETL con Spark u otras herramientas
â”‚   â”‚   â”œâ”€â”€ utils/             # Funciones auxiliares para ingestiÃ³n
â”‚   â”‚   â””â”€â”€ main_data.py       # Punto de entrada del mÃ³dulo de data management
â”‚   â”‚
â”‚   â”œâ”€â”€ data_analytics/        # MÃ³dulo 2ï¸âƒ£: Consultas y modelos de analÃ­tica
â”‚   â”‚   â”œâ”€â”€ querying/          # Consultas SQL/DuckDB sobre Delta Lake
â”‚   â”‚   â”œâ”€â”€ models/            # Modelos ML/NN para analÃ­tica avanzada
â”‚   â”‚   â”œâ”€â”€ utils/             # Funciones auxiliares para analÃ­tica
â”‚   â”‚   â””â”€â”€ main_analytics.py  # Punto de entrada del mÃ³dulo de analytics
â”‚   â”‚
â”‚   â”œâ”€â”€ visualization/         # MÃ³dulo 3ï¸âƒ£: VisualizaciÃ³n y dashboards
â”‚   â”‚   â”œâ”€â”€ dashboard/         # CÃ³digo de Streamlit u otras herramientas
â”‚   â”‚   â”œâ”€â”€ plots/             # Scripts para generar grÃ¡ficos estÃ¡ticos
â”‚   â”‚   â”œâ”€â”€ utils/             # Funciones auxiliares para visualizaciÃ³n
â”‚   â”‚   â””â”€â”€ main_visualization.py # Punto de entrada del mÃ³dulo de visualizaciÃ³n
â”‚   â”‚
â”‚   â””â”€â”€ utils/                 # Utilidades globales para todo el proyecto
â”‚       â””â”€â”€ logging_setup.py   # ConfiguraciÃ³n centralizada de logs
â”‚
â””â”€â”€ tests/                     # Pruebas unitarias por mÃ³dulo
```
---

## ğŸ“Š Flujo de Datos â€“ Proyecto Climascan

El pipeline de datos sigue un enfoque **medallion architecture** (Bronze â†’ Silver â†’ Gold), adaptado a nuestro proyecto:


### ğŸ”¹ Landing
- **raw_aemet/** â†’ datos brutos descargados desde la API AEMET.  
- **raw_claims/** â†’ datos iniciales de siniestros.

### ğŸ”¹ Trusted
- **trusted/aemet_deltalake/** â†’ datos de AEMET limpios, transformados y normalizados.  
- **trusted/claims/weather_claims.parquet** â†’ siniestros estandarizados y enriquecidos con centroides de CP.

### ğŸ”¹ Aggregated
- **aggregated/claims_enriched.parquet** â†’ dataset final enriquecido con interpolaciÃ³n k-NN de variables meteorolÃ³gicas.


### ğŸ”¹ Data Sources External
- **GeoJSON CÃ³digos Postales** â†’ centroides de cÃ³digos postales para georreferenciaciÃ³n.

Los archivos de cÃ³digos postales por provincia utilizados en este proyecto se obtienen desde el repositorio pÃºblico de [ÃÃ±igo Flores](https://github.com/inigoflores/ds-codigos-postales).
Para reproducir la descarga de los archivos `.geojson`, puede ejecutarse el siguiente script en Python:

```python
import os
import requests

provincias = [
    "A_CORUNA", "ALAVA", "ALBACETE", "ALICANTE", "ALMERIA", "ASTURIAS", "AVILA", "BADAJOZ",
    "BALEARES", "BARCELONA", "BURGOS", "CACERES", "CADIZ", "CANTABRIA", "CASTELLON",
    "CEUTA", "CIUDAD_REAL", "CORDOBA", "CUENCA", "GIRONA", "GRANADA", "GUADALAJARA",
    "GIPUZCOA", "HUELVA", "HUESCA", "JAEN", "LA_RIOJA", "LAS_PALMAS",
    "LEON", "LLEIDA", "LUGO", "MADRID", "MALAGA", "MELILLA", "MURCIA", "NAVARRA",
    "OURENSE", "PALENCIA", "PONTEVEDRA", "SALAMANCA",
    "SEGOVIA", "SEVILLA", "SORIA", "TARRAGONA", "TERUEL", "TENERIFE",
    "TOLEDO", "VALENCIA", "VALLADOLID", "VIZCAYA", "ZAMORA", "ZARAGOZA"
]

BASE_URL = "https://raw.githubusercontent.com/inigoflores/ds-codigos-postales/master/data/"
DEST_DIR = "data/external/data"
os.makedirs(DEST_DIR, exist_ok=True)

for prov in provincias:
    file_url = f"{BASE_URL}{prov}.geojson"
    dest_path = os.path.join(DEST_DIR, f"{prov}.geojson")
    try:
        print(f"Descargando {prov}...", end=" ")
        r = requests.get(file_url, timeout=30)
        if r.status_code == 200:
            with open(dest_path, "wb") as f:
                f.write(r.content)
            print("OK")
        else:
            print(f"Error HTTP {r.status_code}")
    except Exception as e:
        print(f"Error con {prov}: {e}")

print("\n Descarga completada.")
```

Los archivos obtenidos deben ubicarse dentro del repositorio, en ```
data/external/data```.

---

### ğŸ”„ Flujo resumido
1. **Ingesta** â†’ descarga de datos desde APIs y archivos externos.  
2. **Landing** â†’ almacenamiento en bruto (raw).  
3. **Trusted** â†’ limpieza, normalizaciÃ³n y uniÃ³n con centroides CP.  
4. **Aggregated** â†’ interpolaciÃ³n espacial (k-NN) y enriquecimiento final.  
