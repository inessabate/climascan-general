{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# **Análisis exploratorio de calidad datos meteorológicos (AEMET)**",
   "id": "562fb17dd4569eb9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Índice del Análisis Exploratorio de Calidad de Datos (Landing → Trusted)\n",
    "\n",
    "## A. Datos generales\n",
    "- A.1 Número de registros totales en el dataset\n",
    "- A.2 Check de duplicados (`fecha` + `indicativo`)\n",
    "- A.3 Número de registros por estación (cobertura)\n",
    "  - Clasificación de estaciones por cobertura:\n",
    "    - Alta (>90% de días con registros)\n",
    "    - Media (80–90%)\n",
    "    - Baja (<80%)\n",
    "  - Evaluación de si el rango alto (90–100%) es aceptable\n",
    "- A.4 Evaluación de estaciones de cobertura media y baja:\n",
    "  - Distribución espacial (ubicación en el territorio)\n",
    "  - Distribución temporal (¿faltantes dispersos o concentrados en periodos específicos?)\n",
    "- A.5 Número de registros por estación y por año (con foco en estaciones de cobertura alta)\n",
    "\n",
    "## B. Tipos de datos y formatos\n",
    "- B.1 Valores numéricos: transformación de coma decimal a punto decimal (`float`)\n",
    "- B.2 Fechas: unificación en formato `YYYY-MM-DD` y validación de fechas reales y plausibles\n",
    "  - Sin fechas futuras fuera del rango esperado\n",
    "- B.3 Horas: unificación a formato `HH:MM`\n",
    "- B.4 Normalización de texto: provincias y nombres de estaciones (mayúsculas/minúsculas, tildes)\n",
    "- B.5 Validación de unidades de medida (temperatura en ºC, precipitación en mm, etc.)\n",
    "\n",
    "## C. Completitud\n",
    "- C.1 Definición de campos obligatorios (`NOT NULL`)\n",
    "- C.2 Política de tratamiento de nulos (imputación vs descarte)\n",
    "- C.3 Análisis de missingness: detección de concentraciones de valores nulos en periodos concretos (ej. todas las estaciones sin `tmax` en un mes → posible fallo de ingestión)\n",
    "- C.4 Cobertura temporal: validación del rango de fechas y continuidad de series por estación\n",
    "- C.5 Cobertura espacial: validación del inventario de estaciones (unicidad de `indicativo`, coherencia de coordenadas)\n",
    "\n",
    "## D. Reglas de rango y consistencia\n",
    "- D.1 Coherencia entre variables relacionadas:\n",
    "  - `tmin ≤ tmed ≤ tmax`\n",
    "  - `hrmin ≤ hrmedia ≤ hrmax`\n",
    "  - `presmin ≤ presmax`\n",
    "- D.2 Valores dentro de rangos físicos plausibles en España:\n",
    "  - Temperatura: [–40, 50] ºC\n",
    "  - Precipitación: ≥ 0\n",
    "  - Humedad relativa: [0, 100]\n",
    "  - Altitud: [–100, 4000]\n",
    "  - Dirección de viento: [0, 360] o códigos especiales (88, 99)\n",
    "- D.3 Validación del resto de campos según su rango esperado\n",
    "\n",
    "## E. Detección de outliers\n",
    "- E.1 Identificación de valores atípicos mediante técnicas estadísticas (z-score, IQR)\n",
    "- E.2 Marcado de outliers (no eliminación en esta fase)\n",
    "- E.3 Notas: los outliers se analizarán en la fase de modelado para determinar si corresponden a fenómenos meteorológicos extremos reales\n",
    "\n",
    "## F. Homogeneización y enriquecimiento\n",
    "- F.1 Inclusión de un campo `fuente_datos` para trazabilidad del origen\n",
    "- F.2 Generación de un flag `dato_imputado` para distinguir valores originales de estimados"
   ],
   "id": "72ae8964e1993c1a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Uso de Apache Spark para el análisis exploratorio\n",
    "\n",
    "El análisis exploratorio de los datos meteorológicos requiere procesar un volumen elevado de información: **2.848.554 registros** correspondientes a observaciones diarias de múltiples estaciones a lo largo de un periodo de casi nueve años.\n",
    "Ante este escenario, resulta fundamental seleccionar una tecnología que garantice eficiencia, escalabilidad y capacidad de integración con el resto de la arquitectura del *Data Lake*.\n",
    "\n",
    "Se ha optado por emplear **Apache Spark** en lugar de alternativas como **DuckDB**."
   ],
   "id": "a2ee3b43b14945bf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Configuración inicial:\n",
    " 1. Creación de sesión Spark\n",
    "2. Obtención del dataset"
   ],
   "id": "c218677c94da1660"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T17:07:02.761420Z",
     "start_time": "2025-08-21T17:07:00.183179Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Crear builder\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "DELTA_VERSION = \"3.2.0\"  # compatible con Spark 3.5.x (Scala 2.12)\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"EDA_Calidad_Landing\")\n",
    "    # Extensiones Delta\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    # 👉 Añadir los JAR de Delta desde Maven Central\n",
    "    .config(\"spark.jars.packages\", f\"io.delta:delta-spark_2.12:{DELTA_VERSION}\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"Spark:\", spark.version)\n",
    "print(\"Extensiones:\", spark.conf.get(\"spark.sql.extensions\"))\n",
    "print(\"Packages:\", spark.sparkContext.getConf().get(\"spark.jars.packages\"))"
   ],
   "id": "c978d3c08d76a7b8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/21 19:07:01 WARN Utils: Your hostname, MacBook-Pro-de-Ines.local resolves to a loopback address: 127.0.0.1; using 172.20.10.3 instead (on interface en0)\n",
      "25/08/21 19:07:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /Users/inessabate/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/inessabate/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-371fe86e-627c-402b-9081-f47436a9fdd1;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/inessabate/PycharmProjects/climascan-general/.venv311/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound io.delta#delta-spark_2.12;3.2.0 in central\n",
      "\tfound io.delta#delta-storage;3.2.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 85ms :: artifacts dl 2ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.12;3.2.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.2.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-371fe86e-627c-402b-9081-f47436a9fdd1\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/2ms)\n",
      "25/08/21 19:07:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/21 19:07:02 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark: 3.5.1\n",
      "Extensiones: io.delta.sql.DeltaSparkSessionExtension\n",
      "Packages: io.delta:delta-spark_2.12:3.2.0\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T17:13:06.905875Z",
     "start_time": "2025-08-21T17:13:06.691445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Obtección del dataset\n",
    "from pathlib import Path\n",
    "\n",
    "# Root del proyecto: subir dos niveles desde notebooks/01_data_analytics/\n",
    "PROJECT_ROOT = Path().resolve().parents[1]\n",
    "LANDING_PATH = PROJECT_ROOT / \"data\" / \"landing\" / \"aemet_deltalake\"\n",
    "\n",
    "try:\n",
    "    df = spark.read.format(\"delta\").load(str(LANDING_PATH))   # si hay _delta_log\n",
    "except Exception:\n",
    "    df = spark.read.parquet(str(LANDING_PATH / \"*.parquet\"))  # fallback parquet\n",
    "\n",
    "print(\"Filas:\", df.count(), \"| Columnas:\", len(df.columns))\n",
    "\n",
    "print(\"Schema df:\")\n",
    "df.printSchema()"
   ],
   "id": "fb3873a370415d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas: 2682447 | Columnas: 26\n",
      "Schema df:\n",
      "root\n",
      " |-- fecha: string (nullable = true)\n",
      " |-- indicativo: string (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- provincia: string (nullable = true)\n",
      " |-- altitud: string (nullable = true)\n",
      " |-- tmed: string (nullable = true)\n",
      " |-- prec: string (nullable = true)\n",
      " |-- tmin: string (nullable = true)\n",
      " |-- horatmin: string (nullable = true)\n",
      " |-- tmax: string (nullable = true)\n",
      " |-- horatmax: string (nullable = true)\n",
      " |-- hrMax: string (nullable = true)\n",
      " |-- horaHrMax: string (nullable = true)\n",
      " |-- hrMin: string (nullable = true)\n",
      " |-- horaHrMin: string (nullable = true)\n",
      " |-- hrMedia: string (nullable = true)\n",
      " |-- dir: string (nullable = true)\n",
      " |-- velmedia: string (nullable = true)\n",
      " |-- racha: string (nullable = true)\n",
      " |-- horaracha: string (nullable = true)\n",
      " |-- presMax: string (nullable = true)\n",
      " |-- horaPresMax: string (nullable = true)\n",
      " |-- presMin: string (nullable = true)\n",
      " |-- horaPresMin: string (nullable = true)\n",
      " |-- sol: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## A. Datos generales\n",
    "- A.1 Número de registros totales en el dataset\n",
    "- A.2 Check de duplicados (`fecha` + `indicativo`)\n",
    "- A.3 Número de registros por estación (cobertura)\n",
    "  - Clasificación de estaciones por cobertura:\n",
    "    - Alta (>90% de días con registros)\n",
    "    - Media (80–90%)\n",
    "    - Baja (<80%)\n",
    "  - Evaluación de si el rango alto (90–100%) es aceptable\n",
    "- A.4 Evaluación de estaciones de cobertura media y baja:\n",
    "  - Distribución espacial (ubicación en el territorio)\n",
    "  - Distribución temporal (¿faltantes dispersos o concentrados en periodos específicos?)\n",
    "- A.5 Número de registros por estación y por año (con foco en estaciones de cobertura alta)"
   ],
   "id": "f71a52a185513ba6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### A.1 Número de registros totales en el dataset\n",
    "- Numero de registros presentes en el dataset\n",
    "- Numero de registros esperados en total"
   ],
   "id": "9d2c4b526ec4d724"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T17:23:07.872790Z",
     "start_time": "2025-08-21T17:23:06.227690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import col, to_date, min as Fmin, max as Fmax\n",
    "total_registros = df.count()\n",
    "print(\"Total de registros en el dataset:\", f\"{total_registros:,}\")\n",
    "\n",
    "# Rango de fechas del dataset\n",
    "if \"fecha\" in df.columns:\n",
    "    df = df.withColumn(\"fecha\", to_date(col(\"fecha\")))\n",
    "    start_date = df.select(Fmin(\"fecha\")).first()[0]\n",
    "    end_date = df.select(Fmax(\"fecha\")).first()[0]\n",
    "print(f\"Rango fechas: {start_date} → {end_date}\")\n",
    "\n",
    "# Total estaciones\n",
    "total_estaciones = df.select(\"indicativo\").distinct().count()\n",
    "print(\"Total de estaciones:\", f\"{total_estaciones:,}\")"
   ],
   "id": "88bcaf081c30fe8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de registros en el dataset: 2,682,447\n",
      "Rango fechas: 2017-01-01 → 2025-06-30\n",
      "Total de estaciones: 918\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T17:17:28.412542Z",
     "start_time": "2025-08-21T17:17:28.399191Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import col, to_date, year, count, lit, min as Fmin, max as Fmax, avg, stddev\n",
    "from datetime import date\n",
    "\n",
    "# Asegurar que 'fecha' es de tipo DATE\n",
    "if \"fecha\" in df.columns:\n",
    "    df = df.withColumn(\"fecha\", to_date(col(\"fecha\")))\n",
    "\n",
    "# Parámetros del análisis (rango real del dataset)\n",
    "START_DATE = date(2017, 1, 1)\n",
    "END_DATE   = date(2025, 6, 30)\n",
    "\n",
    "# Máximo de registros esperados por estación = días entre START_DATE y END_DATE (incluidos)\n",
    "EXPECTED_MAX_PER_STATION = (END_DATE - START_DATE).days + 1\n",
    "\n",
    "print(f\"Rango analizado: {START_DATE} → {END_DATE}\")\n",
    "print(\"Registros esperados por estación:\", EXPECTED_MAX_PER_STATION)\n",
    "\n",
    "# Total esperado\n",
    "expected_total = total_estaciones * EXPECTED_MAX_PER_STATION\n",
    "print(\"Total esperado (estaciones × registros por estación):\", f\"{expected_total:,}\")"
   ],
   "id": "f66f42b2bbdab16f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rango analizado: 2017-01-01 → 2025-06-30\n",
      "Registros esperados por estación: 3103\n",
      "Total esperado (estaciones × registros por estación): 2,848,554\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "El numero total de registros en el dataset es **2,682,447**. El total esperado es **2,848,554** registros, lo que indica que hay **166,107 registros menos de los esperados**.",
   "id": "afd04867a21ab19b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
