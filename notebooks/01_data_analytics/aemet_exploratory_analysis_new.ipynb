{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# **AnÃ¡lisis exploratorio de calidad datos meteorolÃ³gicos (AEMET)**",
   "id": "562fb17dd4569eb9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Ãndice del AnÃ¡lisis Exploratorio de Calidad de Datos (Landing â†’ Trusted)\n",
    "\n",
    "## A. Datos generales\n",
    "- A.1 NÃºmero de registros totales en el dataset\n",
    "- A.2 Check de duplicados (`fecha` + `indicativo`)\n",
    "- A.3 NÃºmero de registros por estaciÃ³n (cobertura)\n",
    "  - ClasificaciÃ³n de estaciones por cobertura:\n",
    "    - Alta (>90% de dÃ­as con registros)\n",
    "    - Media (80â€“90%)\n",
    "    - Baja (<80%)\n",
    "  - EvaluaciÃ³n de si el rango alto (90â€“100%) es aceptable\n",
    "- A.4 EvaluaciÃ³n de estaciones de cobertura media y baja:\n",
    "  - DistribuciÃ³n espacial (ubicaciÃ³n en el territorio)\n",
    "  - DistribuciÃ³n temporal (Â¿faltantes dispersos o concentrados en periodos especÃ­ficos?)\n",
    "- A.5 NÃºmero de registros por estaciÃ³n y por aÃ±o (con foco en estaciones de cobertura alta)\n",
    "\n",
    "## B. Tipos de datos y formatos\n",
    "- B.1 Valores numÃ©ricos: transformaciÃ³n de coma decimal a punto decimal (`float`)\n",
    "- B.2 Fechas: unificaciÃ³n en formato `YYYY-MM-DD` y validaciÃ³n de fechas reales y plausibles\n",
    "  - Sin fechas futuras fuera del rango esperado\n",
    "- B.3 Horas: unificaciÃ³n a formato `HH:MM`\n",
    "- B.4 NormalizaciÃ³n de texto: provincias y nombres de estaciones (mayÃºsculas/minÃºsculas, tildes)\n",
    "- B.5 ValidaciÃ³n de unidades de medida (temperatura en ÂºC, precipitaciÃ³n en mm, etc.)\n",
    "\n",
    "## C. Completitud\n",
    "- C.1 DefiniciÃ³n de campos obligatorios (`NOT NULL`)\n",
    "- C.2 PolÃ­tica de tratamiento de nulos (imputaciÃ³n vs descarte)\n",
    "- C.3 AnÃ¡lisis de missingness: detecciÃ³n de concentraciones de valores nulos en periodos concretos (ej. todas las estaciones sin `tmax` en un mes â†’ posible fallo de ingestiÃ³n)\n",
    "- C.4 Cobertura temporal: validaciÃ³n del rango de fechas y continuidad de series por estaciÃ³n\n",
    "- C.5 Cobertura espacial: validaciÃ³n del inventario de estaciones (unicidad de `indicativo`, coherencia de coordenadas)\n",
    "\n",
    "## D. Reglas de rango y consistencia\n",
    "- D.1 Coherencia entre variables relacionadas:\n",
    "  - `tmin â‰¤ tmed â‰¤ tmax`\n",
    "  - `hrmin â‰¤ hrmedia â‰¤ hrmax`\n",
    "  - `presmin â‰¤ presmax`\n",
    "- D.2 Valores dentro de rangos fÃ­sicos plausibles en EspaÃ±a:\n",
    "  - Temperatura: [â€“40, 50] ÂºC\n",
    "  - PrecipitaciÃ³n: â‰¥ 0\n",
    "  - Humedad relativa: [0, 100]\n",
    "  - Altitud: [â€“100, 4000]\n",
    "  - DirecciÃ³n de viento: [0, 360] o cÃ³digos especiales (88, 99)\n",
    "- D.3 ValidaciÃ³n del resto de campos segÃºn su rango esperado\n",
    "\n",
    "## E. DetecciÃ³n de outliers\n",
    "- E.1 IdentificaciÃ³n de valores atÃ­picos mediante tÃ©cnicas estadÃ­sticas (z-score, IQR)\n",
    "- E.2 Marcado de outliers (no eliminaciÃ³n en esta fase)\n",
    "- E.3 Notas: los outliers se analizarÃ¡n en la fase de modelado para determinar si corresponden a fenÃ³menos meteorolÃ³gicos extremos reales\n",
    "\n",
    "## F. HomogeneizaciÃ³n y enriquecimiento\n",
    "- F.1 InclusiÃ³n de un campo `fuente_datos` para trazabilidad del origen\n",
    "- F.2 GeneraciÃ³n de un flag `dato_imputado` para distinguir valores originales de estimados"
   ],
   "id": "72ae8964e1993c1a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Uso de Apache Spark para el anÃ¡lisis exploratorio\n",
    "\n",
    "El anÃ¡lisis exploratorio de los datos meteorolÃ³gicos requiere procesar un volumen elevado de informaciÃ³n: **2.848.554 registros** correspondientes a observaciones diarias de mÃºltiples estaciones a lo largo de un periodo de casi nueve aÃ±os.\n",
    "Ante este escenario, resulta fundamental seleccionar una tecnologÃ­a que garantice eficiencia, escalabilidad y capacidad de integraciÃ³n con el resto de la arquitectura del *Data Lake*.\n",
    "\n",
    "Se ha optado por emplear **Apache Spark** en lugar de alternativas como **DuckDB**."
   ],
   "id": "a2ee3b43b14945bf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ConfiguraciÃ³n inicial:\n",
    " 1. CreaciÃ³n de sesiÃ³n Spark\n",
    "2. ObtenciÃ³n del dataset"
   ],
   "id": "c218677c94da1660"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T17:07:02.761420Z",
     "start_time": "2025-08-21T17:07:00.183179Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Crear builder\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "DELTA_VERSION = \"3.2.0\"  # compatible con Spark 3.5.x (Scala 2.12)\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"EDA_Calidad_Landing\")\n",
    "    # Extensiones Delta\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    # ğŸ‘‰ AÃ±adir los JAR de Delta desde Maven Central\n",
    "    .config(\"spark.jars.packages\", f\"io.delta:delta-spark_2.12:{DELTA_VERSION}\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"Spark:\", spark.version)\n",
    "print(\"Extensiones:\", spark.conf.get(\"spark.sql.extensions\"))\n",
    "print(\"Packages:\", spark.sparkContext.getConf().get(\"spark.jars.packages\"))"
   ],
   "id": "c978d3c08d76a7b8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/21 19:07:01 WARN Utils: Your hostname, MacBook-Pro-de-Ines.local resolves to a loopback address: 127.0.0.1; using 172.20.10.3 instead (on interface en0)\n",
      "25/08/21 19:07:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /Users/inessabate/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/inessabate/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-371fe86e-627c-402b-9081-f47436a9fdd1;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/inessabate/PycharmProjects/climascan-general/.venv311/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound io.delta#delta-spark_2.12;3.2.0 in central\n",
      "\tfound io.delta#delta-storage;3.2.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 85ms :: artifacts dl 2ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.12;3.2.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.2.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-371fe86e-627c-402b-9081-f47436a9fdd1\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/2ms)\n",
      "25/08/21 19:07:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/21 19:07:02 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark: 3.5.1\n",
      "Extensiones: io.delta.sql.DeltaSparkSessionExtension\n",
      "Packages: io.delta:delta-spark_2.12:3.2.0\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T17:13:06.905875Z",
     "start_time": "2025-08-21T17:13:06.691445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ObtecciÃ³n del dataset\n",
    "from pathlib import Path\n",
    "\n",
    "# Root del proyecto: subir dos niveles desde notebooks/01_data_analytics/\n",
    "PROJECT_ROOT = Path().resolve().parents[1]\n",
    "LANDING_PATH = PROJECT_ROOT / \"data\" / \"landing\" / \"aemet_deltalake\"\n",
    "\n",
    "try:\n",
    "    df = spark.read.format(\"delta\").load(str(LANDING_PATH))   # si hay _delta_log\n",
    "except Exception:\n",
    "    df = spark.read.parquet(str(LANDING_PATH / \"*.parquet\"))  # fallback parquet\n",
    "\n",
    "print(\"Filas:\", df.count(), \"| Columnas:\", len(df.columns))\n",
    "\n",
    "print(\"Schema df:\")\n",
    "df.printSchema()"
   ],
   "id": "fb3873a370415d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas: 2682447 | Columnas: 26\n",
      "Schema df:\n",
      "root\n",
      " |-- fecha: string (nullable = true)\n",
      " |-- indicativo: string (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- provincia: string (nullable = true)\n",
      " |-- altitud: string (nullable = true)\n",
      " |-- tmed: string (nullable = true)\n",
      " |-- prec: string (nullable = true)\n",
      " |-- tmin: string (nullable = true)\n",
      " |-- horatmin: string (nullable = true)\n",
      " |-- tmax: string (nullable = true)\n",
      " |-- horatmax: string (nullable = true)\n",
      " |-- hrMax: string (nullable = true)\n",
      " |-- horaHrMax: string (nullable = true)\n",
      " |-- hrMin: string (nullable = true)\n",
      " |-- horaHrMin: string (nullable = true)\n",
      " |-- hrMedia: string (nullable = true)\n",
      " |-- dir: string (nullable = true)\n",
      " |-- velmedia: string (nullable = true)\n",
      " |-- racha: string (nullable = true)\n",
      " |-- horaracha: string (nullable = true)\n",
      " |-- presMax: string (nullable = true)\n",
      " |-- horaPresMax: string (nullable = true)\n",
      " |-- presMin: string (nullable = true)\n",
      " |-- horaPresMin: string (nullable = true)\n",
      " |-- sol: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## A. Datos generales\n",
    "- A.1 NÃºmero de registros totales en el dataset\n",
    "- A.2 Check de duplicados (`fecha` + `indicativo`)\n",
    "- A.3 NÃºmero de registros por estaciÃ³n (cobertura)\n",
    "  - ClasificaciÃ³n de estaciones por cobertura:\n",
    "    - Alta (>90% de dÃ­as con registros)\n",
    "    - Media (80â€“90%)\n",
    "    - Baja (<80%)\n",
    "  - EvaluaciÃ³n de si el rango alto (90â€“100%) es aceptable\n",
    "- A.4 EvaluaciÃ³n de estaciones de cobertura media y baja:\n",
    "  - DistribuciÃ³n espacial (ubicaciÃ³n en el territorio)\n",
    "  - DistribuciÃ³n temporal (Â¿faltantes dispersos o concentrados en periodos especÃ­ficos?)\n",
    "- A.5 NÃºmero de registros por estaciÃ³n y por aÃ±o (con foco en estaciones de cobertura alta)"
   ],
   "id": "f71a52a185513ba6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### A.1 NÃºmero de registros totales en el dataset\n",
    "- Numero de registros presentes en el dataset\n",
    "- Numero de registros esperados en total"
   ],
   "id": "9d2c4b526ec4d724"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T17:23:07.872790Z",
     "start_time": "2025-08-21T17:23:06.227690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import col, to_date, min as Fmin, max as Fmax\n",
    "total_registros = df.count()\n",
    "print(\"Total de registros en el dataset:\", f\"{total_registros:,}\")\n",
    "\n",
    "# Rango de fechas del dataset\n",
    "if \"fecha\" in df.columns:\n",
    "    df = df.withColumn(\"fecha\", to_date(col(\"fecha\")))\n",
    "    start_date = df.select(Fmin(\"fecha\")).first()[0]\n",
    "    end_date = df.select(Fmax(\"fecha\")).first()[0]\n",
    "print(f\"Rango fechas: {start_date} â†’ {end_date}\")\n",
    "\n",
    "# Total estaciones\n",
    "total_estaciones = df.select(\"indicativo\").distinct().count()\n",
    "print(\"Total de estaciones:\", f\"{total_estaciones:,}\")"
   ],
   "id": "88bcaf081c30fe8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de registros en el dataset: 2,682,447\n",
      "Rango fechas: 2017-01-01 â†’ 2025-06-30\n",
      "Total de estaciones: 918\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T17:17:28.412542Z",
     "start_time": "2025-08-21T17:17:28.399191Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import col, to_date, year, count, lit, min as Fmin, max as Fmax, avg, stddev\n",
    "from datetime import date\n",
    "\n",
    "# Asegurar que 'fecha' es de tipo DATE\n",
    "if \"fecha\" in df.columns:\n",
    "    df = df.withColumn(\"fecha\", to_date(col(\"fecha\")))\n",
    "\n",
    "# ParÃ¡metros del anÃ¡lisis (rango real del dataset)\n",
    "START_DATE = date(2017, 1, 1)\n",
    "END_DATE   = date(2025, 6, 30)\n",
    "\n",
    "# MÃ¡ximo de registros esperados por estaciÃ³n = dÃ­as entre START_DATE y END_DATE (incluidos)\n",
    "EXPECTED_MAX_PER_STATION = (END_DATE - START_DATE).days + 1\n",
    "\n",
    "print(f\"Rango analizado: {START_DATE} â†’ {END_DATE}\")\n",
    "print(\"Registros esperados por estaciÃ³n:\", EXPECTED_MAX_PER_STATION)\n",
    "\n",
    "# Total esperado\n",
    "expected_total = total_estaciones * EXPECTED_MAX_PER_STATION\n",
    "print(\"Total esperado (estaciones Ã— registros por estaciÃ³n):\", f\"{expected_total:,}\")"
   ],
   "id": "f66f42b2bbdab16f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rango analizado: 2017-01-01 â†’ 2025-06-30\n",
      "Registros esperados por estaciÃ³n: 3103\n",
      "Total esperado (estaciones Ã— registros por estaciÃ³n): 2,848,554\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "El numero total de registros en el dataset es **2,682,447**. El total esperado es **2,848,554** registros, lo que indica que hay **166,107 registros menos de los esperados**.",
   "id": "afd04867a21ab19b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
